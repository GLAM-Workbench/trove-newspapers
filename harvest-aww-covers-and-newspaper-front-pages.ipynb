{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Harvesting Australian Women's Weekly covers\n",
    "### (or all the front pages of any digitised newspaper)\n",
    "\n",
    "Somewhat confusingly, the *Australian Women's Weekly* is in with Trove's digitised newspapers and not the rest of the magazines. There are notebooks in the GLAM Workbench's journals section to help [harvest all of a journal's covers](https://glam-workbench.github.io/trove-journals/#get-covers-or-any-other-pages-from-a-digitised-journal-in-trove) as images, so I thought I should do the same for the Weekly. \n",
    "\n",
    "Just change the `TITLE_ID`, `START_DATE`, `END_DATE`, and `PREFIX`, to harvest all the front pages of any digitised newspaper.\n",
    "\n",
    "## Harvest summary\n",
    "\n",
    "* The list of issues harvested is available [in this CSV](https://glam-workbench.net/trove-newspapers/csv-aww-issues/).\n",
    "* 2,566 images were downloaded.\n",
    "* For easy browsing, I've compiled the images into a set of PDF files, one for each decade, available from Dropbox:\n",
    "  * [1933 to 1939](https://www.dropbox.com/s/0j6zpeuw6tbey5k/aww-1933-1939.pdf?dl=0)\n",
    "  * [1940 to 1949](https://www.dropbox.com/s/y1he8dd6h655weu/aww-1940-1949.pdf?dl=0)\n",
    "  * [1950 to 1959](https://www.dropbox.com/s/i9gp9i51nofmlqo/aww-1950-1959.pdf?dl=0)\n",
    "  * [1960 to 1969](https://www.dropbox.com/s/2of63tovcnphijo/aww-1960-1969.pdf?dl=0)\n",
    "  * [1970 to 1979](https://www.dropbox.com/s/f2yxpg8u4dx5uf2/aww-1970-1979.pdf?dl=0)\n",
    "  * [1980 to 1982](https://www.dropbox.com/s/xanohtas1fi7eu4/aww-1980-1982.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import FileLink, display\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests_cache.CachedSession(\"front_pages\")\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Set some options\n",
    "\n",
    "Modify the values below as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert your Trove API key\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "\n",
    "# Use api key value from environment variables if it is available\n",
    "if os.getenv(\"TROVE_API_KEY\"):\n",
    "    API_KEY = os.getenv(\"TROVE_API_KEY\")\n",
    "\n",
    "# The id of the newspaper you want to harvest\n",
    "TITLE_ID = \"112\"  # 112 is the AWW\n",
    "\n",
    "# Range of years to harvest\n",
    "START_YEAR = 1933\n",
    "END_YEAR = 1983\n",
    "\n",
    "# A prefix to use in file names, if None then the title_id will be used\n",
    "PREFIX = \"aww\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TITLE_URL = f\"https://api.trove.nla.gov.au/v3/newspaper/title/{TITLE_ID}\"\n",
    "\n",
    "\n",
    "def get_current_year(years, year):\n",
    "    \"\"\"\n",
    "    Get data for the current year from the dictionary of years.\n",
    "    \"\"\"\n",
    "    for year_data in years:\n",
    "        if year_data[\"date\"] == str(year):\n",
    "            return year_data\n",
    "\n",
    "\n",
    "def get_issues():\n",
    "    \"\"\"\n",
    "    Get all the issue details by looping through the range of years.\n",
    "    Returns a list of issues.\n",
    "    \"\"\"\n",
    "    params = {\"encoding\": \"json\", \"include\": \"years\"}\n",
    "    headers = {\"X-API-KEY\": API_KEY}\n",
    "    issues = []\n",
    "    for year in tqdm(range(START_YEAR, END_YEAR), desc=\"Issues\"):\n",
    "        # Setting 'range' tells the API to give us a list of issue dates & urls within that date range\n",
    "        date_range = f\"{year}0101-{year}1231\"\n",
    "        params[\"range\"] = date_range\n",
    "        # Get the data\n",
    "        response = s.get(TITLE_URL, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "        # Extract the details for the current year\n",
    "        year_data = get_current_year(data[\"year\"], year)\n",
    "        # Save issue details\n",
    "        for issue in year_data[\"issue\"]:\n",
    "            issues.append(issue)\n",
    "    return issues\n",
    "\n",
    "\n",
    "def get_file_prefix():\n",
    "    \"\"\"\n",
    "    Set the prefix to be used in filenames and data directory.\n",
    "    Defaults to title id if prefix is not set\n",
    "    \"\"\"\n",
    "    if PREFIX:\n",
    "        file_prefix = PREFIX\n",
    "    else:\n",
    "        file_prefix = TITLE_ID\n",
    "    return file_prefix\n",
    "\n",
    "\n",
    "def create_output_dir(file_prefix):\n",
    "    \"\"\"\n",
    "    Create output directory.\n",
    "    \"\"\"\n",
    "    dir_path = Path(\"data\", file_prefix)\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    return dir_path\n",
    "\n",
    "\n",
    "def download_page(page_id, size, file_path):\n",
    "    \"\"\"\n",
    "    Download page image using the supplied id.\n",
    "    Size range is 1 to 7 (7 being the highest res)\n",
    "    \"\"\"\n",
    "    # Format the page url ising the page id\n",
    "    page_url = (\n",
    "        f\"http://trove.nla.gov.au/ndp/imageservice/nla.news-page{page_id}/level{size}\"\n",
    "    )\n",
    "    # Download the image\n",
    "    response = s.get(page_url)\n",
    "    file_path.write_bytes(response.content)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "def harvest_covers(size=5, sample_size=None):\n",
    "    \"\"\"\n",
    "    Get a list of issues of the title.\n",
    "    Loop through the issues downloading each front page/cover.\n",
    "    Return issue metadata.\n",
    "    \"\"\"\n",
    "    # Get a list of issues\n",
    "    issues = get_issues()\n",
    "    # Loop through the issues\n",
    "    for issue in tqdm(issues[:sample_size], desc=\"Pages\"):\n",
    "        # Request the issue url\n",
    "        response = s.get(issue[\"url\"])\n",
    "        # The issue url will be redirected to a page url\n",
    "        # Extract the page id from the page url\n",
    "        page_id = re.search(r\"(\\d+)$\", response.url).group(1)\n",
    "        # Save page id to metadata\n",
    "        issue[\"page_id\"] = page_id\n",
    "        # Set up dirs and files\n",
    "        file_prefix = get_file_prefix()\n",
    "        dir_path = create_output_dir(file_prefix)\n",
    "        file_path = Path(\n",
    "            dir_path,\n",
    "            f'{file_prefix}-{issue[\"date\"].replace(\"-\", \"\")}-page{page_id}.jpg',\n",
    "        )\n",
    "        # If the image hasn't already been downloaded, then download it!\n",
    "        if not file_path.exists():\n",
    "            download_page(page_id, size, file_path)\n",
    "        # Save the image name to the metadata\n",
    "        issue[\"image_name\"] = file_path.name\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Run the harvest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "issues = harvest_covers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(issues)\n",
    "df.rename(columns={\"id\": \"issue_id\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "file_prefix = get_file_prefix()\n",
    "df.to_csv(f\"data/{file_prefix}-issues.csv\", index=False)\n",
    "display(FileLink(f\"data/{file_prefix}-issues.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOR TESTING ONLY -- IGNORE THIS CELL\n",
    "if os.getenv(\"GW_STATUS\") == \"dev\":\n",
    "    PREFIX = \"test\"\n",
    "    issues = harvest_covers(sample_size=5)\n",
    "    assert len(list(Path(\"data\", \"test\").glob(\"*.jpg\"))) == 5\n",
    "    shutil.rmtree(Path(\"data\", \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) for the [GLAM Workbench](https://glam-workbench.github.io/).  \n",
    "Support this project by becoming a [GitHub sponsor](https://github.com/sponsors/wragge?o=esb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rocrate": {
   "action": [
    {
     "isPartOf": "https://github.com/GLAM-Workbench/aww-data",
     "mainEntityOfPage": "https://glam-workbench.net/trove-newspapers/dataset-aww-covers/",
     "name": "Australian Women's Weekly issues and covers data",
     "query": "TITLE_ID = '112'",
     "result": [
      {
       "description": "This file includes metadata for 2,566 issues of the Australian Women's Weekly from 1933 to 1982.",
       "url": "https://github.com/GLAM-Workbench/aww-data/blob/main/aww-issues.csv"
      },
      {
       "description": "This file contains images of the covers of all digitised issues of the Australian Women's Weekly in Trove.",
       "url": "https://glam-workbench.s3.ap-southeast-2.amazonaws.com/aww-covers.tar.gz"
      }
     ],
     "workExample": [
      {
       "name": "AWW covers 1933 to 1939 (PDF)",
       "url": "https://www.dropbox.com/s/0j6zpeuw6tbey5k/aww-1933-1939.pdf?dl=0"
      },
      {
       "name": "AWW covers 1940 to 1949 (PDF)",
       "url": "https://www.dropbox.com/s/y1he8dd6h655weu/aww-1940-1949.pdf?dl=0"
      },
      {
       "name": "AWW covers 1950 to 1959 (PDF)",
       "url": "https://www.dropbox.com/s/i9gp9i51nofmlqo/aww-1950-1959.pdf?dl=0"
      },
      {
       "name": "AWW covers 1960 to 1969 (PDF)",
       "url": "https://www.dropbox.com/s/2of63tovcnphijo/aww-1960-1969.pdf?dl=0"
      },
      {
       "name": "AWW covers 1970 to 1979 (PDF)",
       "url": "https://www.dropbox.com/s/f2yxpg8u4dx5uf2/aww-1970-1979.pdf?dl=0"
      },
      {
       "name": "AWW covers 1980 to 1982 (PDF)",
       "url": "https://www.dropbox.com/s/xanohtas1fi7eu4/aww-1980-1982.pdf?dl=0"
      }
     ]
    }
   ],
   "author": [
    {
     "mainEntityOfPage": "https://timsherratt.au",
     "name": "Sherratt, Tim",
     "orcid": "https://orcid.org/0000-0001-7956-4498"
    }
   ],
   "category": "Harvesting data",
   "description": "Somewhat confusingly, the Australian Women's Weekly is in with Trove's digitised newspapers and not the rest of the magazines. There are notebooks in the GLAM Workbench's journals section to help harvest all of a journal's covers as images, so I thought I should do the same for the Weekly. This notebook can be easily adjusted to download the front pages of any digitised newspaper.",
   "mainEntityOfPage": "https://glam-workbench.net/trove-newspapers/harvest-aww-covers-and-newspaper-front-pages/",
   "name": "Harvesting Australian Women's Weekly covers (or the front pages of any newspaper)",
   "position": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
